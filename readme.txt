ΚΑΡΛΗΣ ΝΙΚΟΛΑΟΣ 1115201800068
ΧΑΤΖΗΠΑΥΛΟΥ ΜΑΡΙΑ 1115201400223

https://github.com/mariac96/project3


Η εργασία έγινε αποκλειστικά με την χρήση google colab

Α.
Διάβαζουμε το αρχείο και το μετατέπουμε σε dataframe.
το df.shape[0] είναι το πλήθος των γραμμών και το df.shape[1] είναι το πλήθος των στηλών. 

Αφού πρέπει να χωρίσουμε κάθε time series σε 80% training set και 20% test set βρίσκουμε σε ποιο column του dataframe αντιστοιχεί το 80% του time series που είναι το df.shape[1] * 0.8
Αποθηκεύουμε το αποτέλεσμα στην μεταβλητή percentage


Σκεφτήκαμε να εφαρμόσουμε τον κώδικα που μας δόθηκε αλλά αντί για ένα dataframe με μία στήλη με πολλές γραμμές, έχουμε πολλές στήλες με πολλές γραμμές.

Ξεκινήσαμε να κάνουμε iterate κάθε γραμμή του dataframe με το iterrows(). Παρατηρήσαμε ότι με την χρήση του row εμφανίζονταν κάθετα τα δεδομένα της γραμμής, σαν στήλη, επομένως σε κάθε επανάληψη δημιουργούμε ένα temp dataframe που έχει τα time series κάθε γραμμής. Έτσι το dataframe temp είναι σαν αυτό που έχει στο παράδειγμα και μπορούμε δημιουργήσουμε σε κάθε επανάληψη τα αντίστοιχα X_train, y_train και y_test, και τα αποθηκεύουμε σε αντίστοιχες λίστες trainXList, trainYList και testList.

Το scale γίνεται ξεχωριστά σε κάθε γραμμή του dataframe.

Κάνουμε επαναληπτικά fit το model για κάθε Χ_train, y_train που δημιουργήσαμε πριν και έχουμε αποθηκευμένα στις λίστες τις οποίες κάνουμε iterate. 

Το batch_size είναι 1024 που είναι το μεγαλύτερο δυνατό.
Επιλέχθηκε μεγάλο batch_size επειδή κάνουμε πολλές φορές fit το μοντέλο και είναι πιο γρήγορο στην εκπαίδευση με μεγάλο batch_size αλλά επειδή εκπαιδεύεται πολλές φορές θα έχει μεγάλο πρόβλημα στην ακρίβεια του μοντέλου.

Ο αριθμός των epochs είναι 5. Δεν χρειαζόμαστε πολλά epochs διότι εφαρμόζουμε το μοντέλο, 360 φορές για αυτό επιλέξαμε μικρό αριθμό, αφού θα έχουμε 1500 epochs συνολικά

Τέλος κάνουμε visualize τα Real Stock Price και τα Predicted Stock Price για ορισμένες μετοχές


Β. Τα ίδια κάνουμε όπως στο Α.

Εκπαιδεύουμε το μοντέλο κάνοντας iterate την trainList.

Το batch_size είναι 1024 που είναι το μεγαλύτερο δυνατό επειδή κάνουμε πολλές φορές fit το μοντέλο και είναι πιο γρήγορο στην εκπαίδευση αλλά επειδή εκπαιδεύεται πολλές φορές δεν θα έχει μεγάλο πρόβλημα στην ακρίβεια του μοντέλου λόγω του μεγάλου batch_size.

Ο αριθμός των epochs είναι 5. Δεν χρειαζόμαστε πολλά epochs διότι εφαρμόζουμε το μοντέλο, 360 φορές για αυτό επιλέξαμε μικρό αριθμό, αφού θα έχουμε 1500 epochs συνολικά.

Ο αριθμός των epochs είναι 6. Δεν χρειαζόμαστε πολλά epochs διότι εφαρμόζουμε το μοντέλο, 360 φορές για αυτό επιλέξαμε μικρό αριθμό.

Με τα X_train που έχουμε αποθηκευμένα στο trainList υπολογίζουμε τα train_mae_lost για όλα τα X_train, και αποθηκεύουμε τα loss σε μια λίστα.

Κάνουμε plot train_mae_loss όπως στο παράδειγμα αλλά για τα 15 πρώτα train_mae_loss και όχι για 1 όπως στο παράδειγμα.

Βλέποντας το γράφημα διαλέξαμε ως threshold το 2.1 επειδή δεν υπάρχουν πολλα loss μεγαλύτερα από αυτό. Όταν το error είναι μεγαλύτερο από αυτό το θεωρούμε ανωμαλία.

Κάνουμε visualise όπως στο παράδειγμα.

Γ. Κάνουμε ό,τι κάναμε και στα Α, Β δηλαδή διαβάζουμε τα δεδομένα και τα αποθηκεύουμε στο dataframe και μετατρέπουμε τα δεδομένα μας με κατάλληλο τρόπο ώστε να μπορούν να χρησιμοποιηθούν από το πρόγραμμα και αποθηκεύουμε τα X_train  και x_test, σε αντίστοιχες λίστες trainList και testList. Το scale γίνεται ξεχωριστά σε κάθε γραμμή του dataframe.

Αλλάξαμε το activation function σε tanh, διότι έχουμε κανονικοποιήσει τα δεδομένα με το minmaxscaler. Επομένως το tanh είναι καλύτερο από το sigmoid.

Για κάθε X_test στο testList αποθηκεύουμε τα prediction του encoder σε μια encodedList. 

Κάνουμε iterate την encodedList και παίρνουμε τα encoded time series τα κάνουμε dataframe μιας σειράς και το προσθέτουμε σε μια λίστα απο dataframe. Αυτή τη λίστα την κάνουμε το dataframe result.

Προσθέτουμε το id ως πρώτη στήλη χωρίζουμε το dataframe σε query και dataset και τα κάνουμε export ως .csv αρχεία χωρίς επικεφαλίδες και index.


Δ. Πάμε να τρέξουμε την εργασία 2 με το dataset που μας δόθηκε για την 3. 
Αργεί υπερβολικά πολύ το discrete frechet search, μισή ώρα για 4 queries για aυτό και μειώσαμε το μέγεθος του input αρχείου σε 100 γραμμές.
Παρά την μείωση των γραμμών, το πρόγραμμα συνέχισε να αργεί υπερβολικά διότι αν και το average tLSH ήταν 7 sec το average tTrueAverage ήταν 120 sec δηλαδή 2 λεπτά δηλαδή έπαιρνε πάνω από 20 λεπτά όλο το πρόγραμμα.

Σε αυτό το σημείο φάνηκε χρήσιμη η ηλιθιότητα μου, διότι στην αρχή είχα καταλάβει ότι το Δ ερώτημα χρειάζεται το dataset της προηγούμενης εργασίας και το έκανα encoded και έφτιαξα και το queryset. 
Παρατήρησα ότι για το search κομμάτι της εργασίας έβγαζε καλά αποτλέσματα με MAF κοντά στο 1 ή ακόμα και 1, και χρόνους στα δέκατα του δευτερολέπτου και tLSH και tCube 10 φορές ταχύτερους από το tTrue, και άρα δούλευε καλά.

Για το clustering με το LSH Mean Frechet και για τις encoded χρονοσειρές και για τις κανονικές τα cluster έχουν το ίδιο περίπου μέγεθος.

Για το clustering με το LSH Mean Frechet δεν δούλεψε κάλά διότι πήγανε όλα τα σημεία στο ίδιο cluster για την encoded πράγμα αδύνατον.

Για το clustering με το LSH Mean Vector, Hypercube Mean Vector έχω το ίδιο πρόβλημα.

Γενικά φαίνεται ότι όταν έχουμε clustering με mean vector οι χρονοσειρές καταλήγουν όλες στο ίδιο cluster. Εξηγείται διότι έχουν γίνει encode και είναι κοντινοί αριθμοί άρα μπορούν να καταλήξουν στο ίδιο cluster.
Στο clustering με mean curve οι χρονοσειρές γίνονται 2d, και δεν υπάρχει τέτοιο πρόβλημα όπως στο mean vector, και έχουμε αποτελέσματα και για τις encoded χρονοσειρές που μοιάζουν με τις κανονικές, στο μέγεθος των cluster.

