# -*- coding: utf-8 -*-
"""detect.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MD9BLcHc5E4WBqgfheOqXy55pDIDgh6n

ΚΑΡΛΗΣ ΝΙΚΟΛΑΟΣ 1115201800068


ΧΑΤΖΗΠΑΥΛΟΥ ΜΑΡΙΑ 1115201400223
"""

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import tensorflow as tf
from tensorflow import keras
import pandas as pd
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from pandas.plotting import register_matplotlib_converters

from sklearn.preprocessing import StandardScaler

# %matplotlib inline
# %config InlineBackend.figure_format='retina'

register_matplotlib_converters()
sns.set(style='whitegrid', palette='muted', font_scale=1.5)

rcParams['figure.figsize'] = 22, 10

RANDOM_SEED = 42

np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

n = 5

"""Διάβασμα του αρχείου με μετατροπή σε dataframe"""

# read files
df = pd.read_csv('/content/gdrive/MyDrive/nasdaq2007_17.csv', header=None,sep = '\t')

df = df.reset_index(drop=True)

print("Number of rows and columns:", df.shape)

"""το df.shape[0] είναι το πλήθος των γραμμών και το df.shape[1] είναι το πλήθος των στηλών. 

Αφού πρέπει να χωρίσουμε κάθε time series σε 80% training set και 20% test set βρίσκουμε σε ποιο column του dataframe αντιστοιχεί το 80% του time series που είναι το df.shape[1] * 0.8

Αποθηκεύουμε το αποτέλεσμα στην μεταβλητή percentage
"""

rows = df.shape[0]
columns = df.shape[1] - 1

percentage = int(columns * 0.80)

def create_dataset(X, y, time_steps=1):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        v = X.iloc[i:(i + time_steps)].values
        Xs.append(v)        
        ys.append(y.iloc[i + time_steps])
    return np.array(Xs), np.array(ys)

"""Σκεφτήκαμε να εφαρμόσουμε τον κώδικα που μας δόθηκε αλλά αντί για ένα dataframe με μία στήλη με πολλές γραμμές έχουμε πολλές στήλες με πολλές γραμμές.


Ξεκινήσαμε να κάνουμε iterate κάθε γραμμή του dataframe με το iterrows(). Παρατηρήσαμε ότι με την χρήση του row εμφανίζονταν κάθετα τα δεδομένα της γραμμής, σαν στήλη, επομένως σε κάθε επανάληψη δημιουργούμε ένα temp dataframe που έχει τα time series κάθε γραμμής. Έτσι το dataframe temp είναι σαν αυτό που έχει στο παράδειγμα και μπορούμε δημιουργήσουμε σε κάθε επανάληψη τα αντίστοιχα X_train, y_train, X_test και y_test, και τα αποθηκεύουμε σε αντίστοιχες λίστες trainList και testList.

Το scale γίνεται ξεχωριστά σε κάθε γραμμή του dataframe
"""

TIME_STEPS = 30
pd.options.mode.chained_assignment = None  # default='warn'

trainList = []
testList = []

for index, row in df.iterrows():
  temp = pd.DataFrame(row[1:])
  temp.columns = ['close']
  train = temp.iloc[:percentage]
  test = temp.iloc[percentage:]

  scaler = StandardScaler()
  scaler = scaler.fit(train[['close']])

  train['close'] = scaler.transform(train[['close']])
  test['close'] = scaler.transform(test[['close']])
  X_train, y_train = create_dataset(train[['close']], train.close, TIME_STEPS)
  X_test, y_test = create_dataset(test[['close']], test.close, TIME_STEPS)

  trainList.append([X_train, y_train])
  testList.append([X_test, y_test])

print(X_train.shape)

model = keras.Sequential()
model.add(keras.layers.LSTM(
    units=64, 
    input_shape=(X_train.shape[1], X_train.shape[2])
))
model.add(keras.layers.Dropout(rate=0.2))
model.add(keras.layers.RepeatVector(n=X_train.shape[1]))
model.add(keras.layers.LSTM(units=64, return_sequences=True))
model.add(keras.layers.Dropout(rate=0.2))
model.add(keras.layers.TimeDistributed(keras.layers.Dense(units=X_train.shape[2])))
model.compile(loss='mae', optimizer='adam')

"""Εκπαιδεύουμε το μοντέλο κάνοντας iterate την trainList.

Το batch_size είναι 1024 που είναι το μεγαλύτερο δυνατό επειδή κάνουμε πολλές φορές fit το μοντέλο και είναι πιο γρήγορο στην εκπαίδευση αλλά επειδή εκπαιδεύεται πολλές φορές δεν θα έχει μεγάλο πρόβλημα στην ακρίβεια του μοντέλου λόγω του μεγάλου batch_size.

Ο αριθμός των epochs είναι 5. Δεν χρειαζόμαστε πολλά epochs διότι εφαρμόζουμε το μοντέλο, 360 φορές για αυτό επιλέξαμε μικρό αριθμό, αφού θα έχουμε 1500 epochs συνολικά.

Ο αριθμός των epochs είναι 6. Δεν χρειαζόμαστε πολλά epochs διότι εφαρμόζουμε το μοντέλο, 360 φορές για αυτό επιλέξαμε μικρό αριθμό.
"""

for index, mylist in enumerate(trainList):
    X_train = mylist[0]
    y_train = mylist[1]
    history = model.fit(X_train, y_train,
        epochs=6, batch_size=1024, validation_split=0.1, shuffle=False)

# print(trainShape)

# model.save('/content/gdrive/MyDrive/modelB')

plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend();

"""Με τα X_train που έχουμε αποθηκευμένα στο trainList υπολογίζουμε τα train_mae_lost για όλα τα X_train, και αποθηκεύουμε τα loss σε μια λίστα"""

train_mae_loss_list = []
for index, mylist in enumerate(trainList):
    X_train = mylist[0]
    X_train_pred = model.predict(X_train)
    train_mae_loss = np.mean(np.abs(X_train_pred - X_train), axis=1)
    train_mae_loss_list.append(train_mae_loss)

"""Κάνουμε plot train_mae_loss όπως στο παράδειγμα αλλά για τα 15 πρώτα train_mae_loss και όχι για 1 όπως στο παράδειγμα"""

for index, mylist in enumerate(train_mae_loss_list):

    if(index < n):
      sns.distplot(mylist, bins=50, kde=True);

"""Με τα X_test που έχουμε αποθηκευμένα στο trainList υπολογίζουμε τα test_mae_lost, και αποθηκεύουμε τα loss σε μια λίστα"""

test_mae_loss_list = []
for index, mylist in enumerate(testList):
    X_test = mylist[0]
    X_test_pred = model.predict(X_test)
    test_mae_loss = np.mean(np.abs(X_test_pred - X_test), axis=1)
    test_mae_loss_list.append(test_mae_loss)

"""Βλέποντας το γράφημα διαλέξαμε ως threshold το 2.3 επειδή δεν υπάρχουν πολλα loss μεγαλύτερα από αυτό. Όταν το error είναι μεγαλύτερο από αυτό το θεωρούμε ανωμαλία."""

THRESHOLD = 2.3


for index, mylist in enumerate(test_mae_loss_list):

    # temp = pd.DataFrame(df[index][1:])
    temp = pd.DataFrame(df.loc[index][1:])
    temp.columns = ['close']
    train = temp.iloc[:percentage]
    test = temp.iloc[percentage:]
    
    scaler = StandardScaler()
    scaler = scaler.fit(train[['close']])

    train['close'] = scaler.transform(train[['close']])
    test['close'] = scaler.transform(test[['close']])

    test_score_df = pd.DataFrame(index=test[TIME_STEPS:].index)
    test_score_df['loss'] = mylist
    test_score_df['threshold'] = THRESHOLD
    test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold
    test_score_df['close'] = test[TIME_STEPS:].close
    
    plt.plot(test_score_df.index, test_score_df.loss, label='loss')
    plt.plot(test_score_df.index, test_score_df.threshold, label='threshold')
    plt.xticks(rotation=25)
    plt.legend();
    break

for index, mylist in enumerate(test_mae_loss_list):
  if(index < n):
    temp = pd.DataFrame(df.loc[index][1:])
    temp.columns = ['close']
    train = temp.iloc[:percentage]
    test = temp.iloc[percentage:]
    
    # scaler = StandardScaler()
    # scaler = scaler.fit(train[['close']])

    # train['close'] = scaler.transform(train[['close']])
    # test['close'] = scaler.transform(test[['close']])

    test_score_df = pd.DataFrame(index=test[TIME_STEPS:].index)
    test_score_df['loss'] = mylist
    test_score_df['threshold'] = THRESHOLD
    test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold
    test_score_df['close'] = test[TIME_STEPS:].close
    
    anomalies = test_score_df[test_score_df.anomaly == True]
    print(anomalies)
    plt.plot(
      test[TIME_STEPS:].index, 
      test[TIME_STEPS:].close, 
      label='close price'
    );
    sns.scatterplot(
      anomalies.index,
      anomalies.close,
      color=sns.color_palette()[3],
      s=52,
      label='anomaly'
    )
    plt.xticks(rotation=25)
    plt.legend();