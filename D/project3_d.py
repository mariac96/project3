# -*- coding: utf-8 -*-
"""project3_d.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JB1T8Y7G3lC0yb_Nvi7nTkPguIX6F8la

ΚΑΡΛΗΣ ΝΙΚΟΛΑΟΣ 1115201800068


ΧΑΤΖΗΠΑΥΛΟΥ ΜΑΡΙΑ 1115201400223
"""

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
from keras.layers import Input, Dense, Conv1D, MaxPooling1D, UpSampling1D, BatchNormalization, LSTM, RepeatVector
from keras.models import Model
from keras.models import model_from_json
from keras import regularizers
import datetime
import time
import requests as req
import json
import pandas as pd
import pickle
import os
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from tqdm import tqdm
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
# %pylab inline

"""Διάβασμα του αρχείου με μετατροπή σε dataframe"""

# read files
df1 = pd.read_csv('/content/gdrive/MyDrive/nasd_input.csv', header=None,sep = '\t')

df1 = df.reset_index(drop=True)

print("Number of rows and columns:", df1.shape)

df2 = pd.read_csv('/content/gdrive/MyDrive/nasd_query.csv', header=None,sep = '\t')

df2 = df2.reset_index(drop=True)
print("Number of rows and columns:", df2.shape)

print(df2)

print(df1)

frames = [df1, df2]
  
df= pd.concat(frames)
df = df.reset_index(drop=True)
print(df)

"""το df.shape[0] είναι το πλήθος των γραμμών και το df.shape[1] είναι το πλήθος των στηλών. 

Αφού πρέπει να χωρίσουμε κάθε time series σε 80% training set και 20% test set βρίσκουμε σε ποιο column του dataframe αντιστοιχεί το 80% του time series που είναι το df.shape[1] * 0.8

Αποθηκεύουμε το αποτέλεσμα στην μεταβλητή percentage
"""

rows = df.shape[0]
columns = df.shape[1] -1

percentage = int(columns * 0.80)

startdate="01/01/2015"
window_length = 10
encoding_dim = 3
epochs = 100
test_samples = 2000

def mkdate(ts):
    return datetime.datetime.fromtimestamp(
        int(ts)
    ).strftime('%Y-%m-%d')

def plot_examples(stock_input, stock_decoded):
    n = 10  
    plt.figure(figsize=(20, 4))
    for i, idx in enumerate(list(np.arange(0, test_samples, 200))):
        # display original
        ax = plt.subplot(2, n, i + 1)
        if i == 0:
            ax.set_ylabel("Input", fontweight=600)
        else:
            ax.get_yaxis().set_visible(False)
        plt.plot(stock_input[idx])
        ax.get_xaxis().set_visible(False)
        

        # display reconstruction
        ax = plt.subplot(2, n, i + 1 + n)
        if i == 0:
            ax.set_ylabel("Output", fontweight=600)
        else:
            ax.get_yaxis().set_visible(False)
        plt.plot(stock_decoded[idx])
        ax.get_xaxis().set_visible(False)
        
        
def plot_history(history):
    plt.figure(figsize=(15, 5))
    ax = plt.subplot(1, 2, 1)
    plt.plot(history.history["loss"])
    plt.title("Train loss")
    ax = plt.subplot(1, 2, 2)
    plt.plot(history.history["val_loss"])
    plt.title("Test loss")

"""Σκεφτήκαμε να εφαρμόσουμε τον κώδικα που μας δόθηκε αλλά αντί για ένα dataframe με μία στήλη με πολλές γραμμές έχουμε πολλές στήλες με πολλές γραμμές.


Ξεκινήσαμε να κάνουμε iterate κάθε γραμμή του dataframe με το iterrows(). Παρατηρήσαμε ότι με την χρήση του row εμφανίζονταν κάθετα τα δεδομένα της γραμμής, σαν στήλη, επομένως σε κάθε επανάληψη δημιουργούμε ένα temp dataframe που έχει τα time series κάθε γραμμής. 

Έτσι το dataframe temp είναι σαν αυτό που έχει στο παράδειγμα και μπορούμε δημιουργήσουμε σε κάθε επανάληψη τα αντίστοιχα X_train και X_test, και τα αποθηκεύουμε σε αντίστοιχες λίστες trainList και testList.

Το scale γίνεται ξεχωριστά σε κάθε γραμμή του dataframe
"""

scaler = MinMaxScaler()

trainList = []
testList = []

for index, row in df.iterrows():
  temp = pd.DataFrame(row[1:].astype(float))
  temp.columns = ['price']

  temp['pct_change'] = temp.price.pct_change()
  temp['log_ret'] = np.log(temp.price) - np.log(temp.price.shift(1))
  x_train_nonscaled = np.array([temp['log_ret'].values[i-window_length:i].reshape(-1, 1) for i in tqdm(range(window_length+1,len(temp['log_ret'])))])
  x_train = np.array([scaler.fit_transform(temp['log_ret'].values[i-window_length:i].reshape(-1, 1)) for i in tqdm(range(window_length+1,len(temp['log_ret'])))])
  
  x_test = x_train[percentage:]
  x_train = x_train[:percentage]

  x_train = x_train.astype('float32')
  x_test = x_test.astype('float32')

  trainList.append(x_train)
  testList.append(x_test)

"""Το μοντέλο που μας δόθηκε.

Αλλάξαμε το activation function σε tanh, διότι έχουμε κανονικοποιήσει τα δεδομένα με το minmaxscaler. Επομένως το tanh είναι καλύτερο από το sigmoid

"""

input_window = Input(shape=(window_length,1))
x = Conv1D(16, 3, activation="tanh", padding="same")(input_window) # 10 dims
#x = BatchNormalization()(x)
x = MaxPooling1D(2, padding="same")(x) # 5 dims
x = Conv1D(1, 3, activation="tanh", padding="same")(x) # 5 dims
#x = BatchNormalization()(x)
encoded = MaxPooling1D(2, padding="same")(x) # 3 dims

encoder = Model(input_window, encoded)

# 3 dimensions in the encoded layer

x = Conv1D(1, 3, activation="tanh", padding="same")(encoded) # 3 dims
#x = BatchNormalization()(x)
x = UpSampling1D(2)(x) # 6 dims
x = Conv1D(16, 2, activation='tanh')(x) # 5 dims
#x = BatchNormalization()(x)
x = UpSampling1D(2)(x) # 10 dims
decoded = Conv1D(1, 3, activation='sigmoid', padding='same')(x) # 10 dims
autoencoder = Model(input_window, decoded)
autoencoder.summary()

autoencoder.compile(optimizer='adam', loss='binary_crossentropy')


for x_train, x_test in zip(trainList, testList): #10
  history = autoencoder.fit(x_train, x_train, epochs=10, batch_size=1024,
                shuffle=True, validation_data=(x_test, x_test))

"""Για κάθε X_test στο testList αποθηκεύουμε τα prediction του encoder σε μια encodedList"""

encodedList = []
# print(len(decodedList)

for index, X_test in enumerate(testList):
    encoded_stocks = encoder.predict(X_test)
    encodedList.append(encoded_stocks)
    # predicted_stock_price = scaler.inverse_transform(decoded_stocks)

# Calling `save('my_model')` creates a SavedModel folder `my_model`.
autoencoder.save('/content/gdrive/MyDrive/modeld')

encoder.save('/content/gdrive/MyDrive/encoder')

encoder.load_weights('/content/gdrive/MyDrive/encoder')

"""Κάνουμε iterate την encodedList και παίρνουμε τα encoded time series τα κάνουμε dataframe μιας σειράς και το προσθέτουμε σε μια λίστα απο dataframe. Αυτή τη λίστα την κάνουμε το dataframe result.

Προσθέτουμε το id ως πρώτη στήλη χωρίζουμε το dataframe σε query και dataset και τα κάνουμε export ως .csv αρχεία
"""

listDf = []

for d in encodedList:

  mylist = []
  for i_1 in range(d.shape[0]):
    for i_2 in range(d.shape[1]):
      mylist.append(d[i_1][i_2])

  temp = pd.DataFrame(mylist).transpose()

  listDf.append(temp)

result = pd.concat(listDf)
result = result.reset_index(drop=True)

result.insert(loc = 0, column = 'id', value= df.iloc[:, 0])
result.columns = [''] * len(result.columns)
print(result)

query = result.tail(10)
query = query.reset_index(drop=True)
print(query)

dataset = result.head(100)
dataset = dataset.reset_index(drop=True)
print(dataset)

dataset.to_csv("/content/gdrive/MyDrive/datasetd.csv", sep="\t", index=False, header=False)

query.to_csv("/content/gdrive/MyDrive/queryd.csv",  sep="\t", index=False, header=False)